#!/bin/bash

# Local workflow testing script
# Tests the main components of the benchmark-and-release workflow locally

echo "ğŸ§ª Testing Benchmark & Release Workflow Components Locally"
echo "=========================================================="

# Check if we're in the right directory
if [[ ! -f "test/performance_test.py" ]]; then
    echo "âŒ Must be run from project root directory"
    exit 1
fi

# Create test output directory
mkdir -p .github/test-output
cd .github/test-output

echo ""
echo "1ï¸âƒ£ Testing Structure Verification"
echo "--------------------------------"
python3 ../../test/verify_implementations.py > verification_results.txt 2>&1
VERIFY_EXIT=$?

if [[ $VERIFY_EXIT -eq 0 ]]; then
    echo "âœ… Structure verification passed"
else
    echo "âš ï¸ Structure verification had warnings (exit code: $VERIFY_EXIT)"
fi

# Extract counts for testing
EXCELLENT=$(grep -c "ğŸŸ¢.*excellent" verification_results.txt || echo "0")
GOOD=$(grep -c "ğŸŸ¡.*good" verification_results.txt || echo "0")
NEEDS_WORK=$(grep -c "ğŸ”´.*needs_work" verification_results.txt || echo "0")
TOTAL=$((EXCELLENT + GOOD + NEEDS_WORK))

echo "ğŸ“Š Verification Summary:"
echo "   Total: $TOTAL implementations"
echo "   ğŸŸ¢ Excellent: $EXCELLENT"
echo "   ğŸŸ¡ Good: $GOOD"
echo "   ğŸ”´ Needs work: $NEEDS_WORK"

echo ""
echo "2ï¸âƒ£ Testing Performance Benchmark (Limited)"
echo "-------------------------------------------"
echo "Note: Running limited test on Python implementation only for speed"

python3 ../../test/performance_test.py \
    --impl ../../implementations/python \
    --timeout 180 \
    --output performance_report_test.txt \
    --json performance_data_test.json \
    > benchmark_output.txt 2>&1

BENCHMARK_EXIT=$?

if [[ $BENCHMARK_EXIT -eq 0 ]]; then
    echo "âœ… Performance benchmark completed successfully"
else
    echo "âš ï¸ Performance benchmark had issues (exit code: $BENCHMARK_EXIT)"
fi

if [[ -f performance_data_test.json ]]; then
    echo "âœ… Performance JSON data generated"
    # Show basic stats
    IMPLEMENTATIONS=$(python3 -c "import json; data=json.load(open('performance_data_test.json')); print(len(data))")
    echo "   Tested implementations: $IMPLEMENTATIONS"
else
    echo "âŒ Performance JSON data not generated"
fi

echo ""
echo "3ï¸âƒ£ Testing README Update Logic"
echo "------------------------------"

# Create a test README update script
cat > test_readme_update.py << 'EOF'
import json
import re
from datetime import datetime
from pathlib import Path

def test_readme_update():
    """Test the README update logic"""
    
    # Load test performance data
    try:
        with open('performance_data_test.json', 'r') as f:
            performance_data = json.load(f)
    except FileNotFoundError:
        print("âš ï¸ No performance data for README test")
        return False
    
    print(f"âœ… Loaded performance data for {len(performance_data)} implementation(s)")
    
    # Test table generation logic
    status_emoji = {'excellent': 'ğŸŸ¢', 'good': 'ğŸŸ¡', 'needs_work': 'ğŸ”´'}
    
    for impl in performance_data:
        lang = impl.get('language', 'Unknown').title()
        status = impl.get('status', 'unknown')
        
        if status == 'completed':
            errors = len(impl.get('errors', []))
            test_results = impl.get('test_results', {})
            failed_tests = len(test_results.get('failed', []))
            
            if errors == 0 and failed_tests == 0:
                final_status = 'excellent'
            elif errors <= 2 and failed_tests <= 1:
                final_status = 'good'
            else:
                final_status = 'needs_work'
        else:
            final_status = 'needs_work'
        
        emoji = status_emoji.get(final_status, 'âšª')
        timings = impl.get('timings', {})
        build_time = timings.get('build_seconds', 0)
        analyze_time = timings.get('analyze_seconds', 0)
        
        print(f"   {emoji} {lang}: Build {build_time:.1f}s, Analysis {analyze_time:.1f}s")
    
    print("âœ… README table generation logic working")
    return True

if __name__ == "__main__":
    test_readme_update()
EOF

python3 test_readme_update.py

echo ""
echo "4ï¸âƒ£ Testing Version Management Logic"
echo "-----------------------------------"

# Get current version
CURRENT_VERSION=$(cd ../.. && git tag --sort=-version:refname | grep -E '^v[0-9]+\.[0-9]+\.[0-9]+$' | head -1)

if [[ -z "$CURRENT_VERSION" ]]; then
    CURRENT_VERSION="v0.0.0"
    echo "â„¹ï¸ No previous version tags found, would start from $CURRENT_VERSION"
else
    echo "â„¹ï¸ Current version: $CURRENT_VERSION"
fi

# Test version bump logic
if [[ $NEEDS_WORK -eq 0 && $EXCELLENT -gt 10 ]]; then
    VERSION_TYPE="minor"
else
    VERSION_TYPE="patch"
fi

echo "ğŸ“ˆ Suggested version bump: $VERSION_TYPE"

# Calculate new version
IFS='.' read -r -a version_parts <<< "${CURRENT_VERSION#v}"
MAJOR=${version_parts[0]:-0}
MINOR=${version_parts[1]:-0}
PATCH=${version_parts[2]:-0}

case $VERSION_TYPE in
    major)
        MAJOR=$((MAJOR + 1))
        MINOR=0
        PATCH=0
        ;;
    minor)
        MINOR=$((MINOR + 1))
        PATCH=0
        ;;
    patch)
        PATCH=$((PATCH + 1))
        ;;
esac

NEW_VERSION="v$MAJOR.$MINOR.$PATCH"
echo "ğŸ·ï¸ Next version would be: $NEW_VERSION"

echo ""
echo "5ï¸âƒ£ Testing Release Notes Generation"
echo "-----------------------------------"

cat > test_release_notes.md << EOF
# Chess Engine Implementation Benchmark Release $NEW_VERSION

This release contains updated performance benchmarks and status information for all chess engine implementations.

## ğŸ“Š Implementation Status Overview

- ğŸŸ¢ **Excellent**: $EXCELLENT implementations
- ğŸŸ¡ **Good**: $GOOD implementations  
- ğŸ”´ **Needs Work**: $NEEDS_WORK implementations

**Total**: $TOTAL implementations tested

## ğŸš€ What's Updated

- âœ… Complete performance benchmark suite executed
- âœ… Implementation structure verification completed
- âœ… README status table updated with latest results
- âœ… Docker build and test validation

*This would be an automatically generated release.*
EOF

echo "âœ… Release notes generated"
echo "ğŸ“„ Preview:"
head -10 test_release_notes.md

echo ""
echo "6ï¸âƒ£ Summary"
echo "----------"
echo "âœ… Structure verification: Working"
echo "âœ… Performance benchmarking: Working"
echo "âœ… README update logic: Working"
echo "âœ… Version management: Working"
echo "âœ… Release notes generation: Working"

echo ""
echo "ğŸ‰ Local workflow testing completed successfully!"
echo ""
echo "ğŸ“ Test outputs saved in .github/test-output/"
echo "   - verification_results.txt"
echo "   - performance_report_test.txt"
echo "   - performance_data_test.json"
echo "   - benchmark_output.txt"
echo "   - test_release_notes.md"
echo ""
echo "ğŸš€ The workflow should work correctly in GitHub Actions!"

# Return to original directory
cd ../..