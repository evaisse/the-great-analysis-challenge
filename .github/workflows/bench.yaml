name: Benchmark Suite & Release

on:
  push:
    branches: [ master ]
  schedule:
    # Run every Sunday at 6 AM UTC
    - cron: '0 6 * * 0'
  workflow_dispatch:
    inputs:
      version_type:
        description: 'Version bump type'
        required: true
        default: 'patch'
        type: choice
        options:
          - patch
          - minor
          - major

env:
  BENCHMARK_TIMEOUT: 3600  # 1 hour timeout
  PYTHON_VERSION: '3.11'

jobs:
  detect-changes:
    name: Detect Changed Implementations
    runs-on: ubuntu-latest
    outputs:
      changed-implementations: ${{ steps.changes.outputs.implementations }}
      has-changes: ${{ steps.changes.outputs.has-changes }}
      test-mode: ${{ steps.changes.outputs.test_mode }}
      impl-count: ${{ steps.changes.outputs.impl_count }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changed implementations
        id: changes
        run: |
          echo "=== Detecting Changed Implementations ==="
          
          # For scheduled runs or manual triggers, test all implementations
          if [[ "${{ github.event_name }}" == "schedule" || "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "Full test requested (scheduled or manual trigger)"
            
            # Count implementations dynamically
            IMPL_COUNT=$(find implementations -name "Dockerfile" -type f | wc -l)
            
            echo "changed_impls=all" >> $GITHUB_OUTPUT
            echo "test_mode=full" >> $GITHUB_OUTPUT
            echo "impl_count=$IMPL_COUNT" >> $GITHUB_OUTPUT
            echo "has-changes=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For push events, detect which implementations changed
          if [[ "${{ github.event_name }}" == "push" ]]; then
            # Get the previous commit to compare against
            PREV_COMMIT="${{ github.event.before }}"
            
            # If this is the first commit, compare against HEAD~1
            if [[ "$PREV_COMMIT" == "0000000000000000000000000000000000000000" ]]; then
              PREV_COMMIT="HEAD~1"
            fi
            
            echo "Comparing against: $PREV_COMMIT"
            
            # Get changed files in implementations directory
            CHANGED_FILES=$(git diff --name-only $PREV_COMMIT HEAD -- implementations/ || echo "")
            
            if [[ -z "$CHANGED_FILES" ]]; then
              echo "No implementation changes detected"
              echo "changed_impls=none" >> $GITHUB_OUTPUT
              echo "test_mode=none" >> $GITHUB_OUTPUT
              echo "impl_count=0" >> $GITHUB_OUTPUT
              echo "has-changes=false" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "Changed files:"
            echo "$CHANGED_FILES"
            
            # Extract unique implementation directories
            CHANGED_IMPLEMENTATIONS=$(echo "$CHANGED_FILES" | grep -E "^implementations/[^/]+/" | cut -d'/' -f2 | sort -u | tr '\n' ' ')
            
            if [[ -z "$CHANGED_IMPLEMENTATIONS" ]]; then
              echo "No specific implementations changed"
              echo "changed_impls=none" >> $GITHUB_OUTPUT
              echo "test_mode=none" >> $GITHUB_OUTPUT
              echo "impl_count=0" >> $GITHUB_OUTPUT
              echo "has-changes=false" >> $GITHUB_OUTPUT
            else
              echo "Changed implementations: $CHANGED_IMPLEMENTATIONS"
              IMPL_COUNT=$(echo "$CHANGED_IMPLEMENTATIONS" | wc -w)
              echo "changed_impls=$CHANGED_IMPLEMENTATIONS" >> $GITHUB_OUTPUT
              echo "test_mode=selective" >> $GITHUB_OUTPUT
              echo "impl_count=$IMPL_COUNT" >> $GITHUB_OUTPUT
              echo "has-changes=true" >> $GITHUB_OUTPUT
            fi
          fi

  validation:
    name: Run Structure Verification
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.has-changes == 'true'
    outputs:
      excellent-count: ${{ steps.verify.outputs.excellent_count }}
      good-count: ${{ steps.verify.outputs.good_count }}
      needs-work-count: ${{ steps.verify.outputs.needs_work_count }}
      total-count: ${{ steps.verify.outputs.total_count }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run structure verification
        id: verify
        run: |
          echo "=== Running Implementation Structure Verification ==="
          python3 test/verify_implementations.py > verification_results.txt 2>&1
          
          # Count implementations by status
          EXCELLENT=$(grep -c "ðŸŸ¢.*excellent" verification_results.txt || echo "0")
          GOOD=$(grep -c "ðŸŸ¡.*good" verification_results.txt || echo "0")
          NEEDS_WORK=$(grep -c "ðŸ”´.*needs_work" verification_results.txt || echo "0")
          TOTAL=$((EXCELLENT + GOOD + NEEDS_WORK))
          
          echo "excellent_count=$EXCELLENT" >> $GITHUB_OUTPUT
          echo "good_count=$GOOD" >> $GITHUB_OUTPUT
          echo "needs_work_count=$NEEDS_WORK" >> $GITHUB_OUTPUT
          echo "total_count=$TOTAL" >> $GITHUB_OUTPUT
          
          echo "=== Verification Summary ==="
          echo "Total implementations: $TOTAL"
          echo "ðŸŸ¢ Excellent: $EXCELLENT"
          echo "ðŸŸ¡ Good: $GOOD"
          echo "ðŸ”´ Needs work: $NEEDS_WORK"
          
          cat verification_results.txt

  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [detect-changes, validation]
    if: needs.detect-changes.outputs.has-changes == 'true'
    timeout-minutes: 90
    outputs:
      benchmark-success: ${{ steps.benchmark.outputs.success }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install psutil gitpython

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Run performance benchmark suite
        id: benchmark
        run: |
          echo "=== Running Performance Benchmark Suite ==="
          
          TEST_MODE="${{ needs.detect-changes.outputs.test-mode }}"
          CHANGED_IMPLS="${{ needs.detect-changes.outputs.changed-implementations }}"
          IMPL_COUNT="${{ needs.detect-changes.outputs.impl-count }}"
          
          echo "Test mode: $TEST_MODE"
          echo "Implementation count: $IMPL_COUNT"
          
          # Create reports directory
          mkdir -p benchmark_reports
          
          if [[ "$TEST_MODE" == "full" ]]; then
            echo "Running full benchmark suite on all implementations"
            python3 test/performance_test.py \
              --timeout ${{ env.BENCHMARK_TIMEOUT }} \
              --output benchmark_reports/performance_report.txt \
              --json benchmark_reports/performance_data.json \
              > benchmark_reports/benchmark_output.txt 2>&1
          elif [[ "$TEST_MODE" == "selective" ]]; then
            echo "Running selective benchmark on changed implementations: $CHANGED_IMPLS"
            
            # Test each changed implementation individually
            for impl in $CHANGED_IMPLS; do
              echo "Testing implementation: $impl"
              
              if [[ -d "implementations/$impl" ]]; then
                python3 test/performance_test.py \
                  --impl "implementations/$impl" \
                  --timeout 900 \
                  --output "benchmark_reports/performance_report_$impl.txt" \
                  --json "benchmark_reports/performance_data_$impl.json" \
                  >> benchmark_reports/benchmark_output.txt 2>&1
              else
                echo "âš ï¸ Implementation directory not found: implementations/$impl"
              fi
            done
            
            # Combine individual JSON reports
            python3 -c "
import json
import glob
from pathlib import Path

all_results = []
for json_file in glob.glob('benchmark_reports/performance_data_*.json'):
    try:
        with open(json_file, 'r') as f:
            data = json.load(f)
            if isinstance(data, list):
                all_results.extend(data)
            else:
                all_results.append(data)
    except Exception as e:
        print(f'Error reading {json_file}: {e}')

# Save combined results
with open('benchmark_reports/performance_data.json', 'w') as f:
    json.dump(all_results, f, indent=2)

print(f'Combined {len(all_results)} implementation results')
" >> benchmark_reports/benchmark_output.txt 2>&1
            
            # Combine text reports
            cat benchmark_reports/performance_report_*.txt > benchmark_reports/performance_report.txt 2>/dev/null || echo "No text reports to combine"
          fi
          
          BENCHMARK_EXIT_CODE=$?
          echo "success=$([[ $BENCHMARK_EXIT_CODE -eq 0 ]] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
          
          # Extract key metrics for summary
          if [[ -f benchmark_reports/performance_data.json ]]; then
            echo "=== Benchmark completed successfully ==="
          else
            echo "âš ï¸ Performance data JSON not generated"
          fi
          
          # Always show benchmark output
          echo "=== Benchmark Output ==="
          cat benchmark_reports/benchmark_output.txt

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-reports-${{ github.sha }}
          path: |
            benchmark_reports/
            verification_results.txt
          retention-days: 30

  update-readme:
    name: Update README Status
    runs-on: ubuntu-latest
    needs: [detect-changes, validation, benchmark]
    if: needs.detect-changes.outputs.has-changes == 'true' && needs.benchmark.outputs.benchmark-success == 'true'
    outputs:
      readme-changed: ${{ steps.update.outputs.changed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v3
        with:
          name: benchmark-reports-${{ github.sha }}

      - name: Generate updated README status table
        id: update
        run: |
          echo "=== Generating Updated README Status Table ==="
          
          # Create Python script to update README
          cat > update_readme_status.py << 'EOF'
import json
import re
from datetime import datetime
from pathlib import Path

def load_performance_data():
    """Load performance benchmark data"""
    try:
        with open('benchmark_reports/performance_data.json', 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âš ï¸ Performance data not found")
        return []

def classify_implementation_status(impl_data):
    """Classify implementation status based on benchmark results"""
    if impl_data.get('status') != 'completed':
        return 'needs_work'
    
    errors = len(impl_data.get('errors', []))
    test_results = impl_data.get('test_results', {})
    failed_tests = len(test_results.get('failed', []))
    
    if errors == 0 and failed_tests == 0:
        return 'excellent'
    elif errors <= 2 and failed_tests <= 1:
        return 'good'
    else:
        return 'needs_work'

def format_time(seconds):
    """Format time duration"""
    if seconds == 0:
        return "~0s"
    elif seconds < 1:
        return f"~{seconds:.1f}s"
    else:
        return f"~{seconds:.0f}s"

def generate_status_table(performance_data):
    """Generate the implementation status table"""
    
    # Status emoji mapping
    status_emoji = {
        'excellent': 'ðŸŸ¢',
        'good': 'ðŸŸ¡',
        'needs_work': 'ðŸ”´'
    }
    
    # Build table rows
    table_rows = []
    
    for impl in sorted(performance_data, key=lambda x: x.get('language', '')):
        lang = impl.get('language', 'Unknown').title()
        status = classify_implementation_status(impl)
        emoji = status_emoji.get(status, 'âšª')
        
        # Extract timing data
        timings = impl.get('timings', {})
        analyze_time = format_time(timings.get('analyze_seconds', 0))
        build_time = format_time(timings.get('build_seconds', 0))
        
        # Features and compliance
        features = "âœ… Complete" if status != 'needs_work' else "ðŸ”§ Issues"
        makefile = "âœ… Full" if status != 'needs_work' else "âŒ Missing"
        docker_status = "âœ… Working"
        if impl.get('docker', {}).get('build_success') is False:
            docker_status = "ðŸ”§ Build Issue"
        
        # Notes based on performance
        notes_text = {
            'excellent': 'Excellent performance',
            'good': 'Good performance', 
            'needs_work': 'Needs attention'
        }.get(status, '')
        
        # Create table row
        row = f"| {emoji} **{lang}** | {build_time} | {analyze_time} | {features} | {makefile} | {docker_status} | {notes_text} |"
        table_rows.append(row)
    
    # Create complete table
    table_header = """| Language | Build Time | Analysis Time | Features | Makefile | Docker | Notes |
|----------|------------|---------------|----------|----------|--------|-------|"""
    
    return table_header + "\n" + "\n".join(table_rows)

def update_readme_status():
    """Update README.md with latest benchmark results"""
    
    # Load performance data
    performance_data = load_performance_data()
    
    if not performance_data:
        print("âŒ No performance data available")
        return False
    
    # Generate new status table
    new_table = generate_status_table(performance_data)
    
    # Read current README
    readme_path = Path('README.md')
    if not readme_path.exists():
        print("âŒ README.md not found")
        return False
    
    content = readme_path.read_text()
    
    # Find and replace the status table
    pattern = r'(## ðŸ“Š Implementation Status Overview\s*\n\n)(.*?)(\n\n### Status Legend|\n\n##|\n\n\*Build times)'
    
    def replacement(match):
        return match.group(1) + new_table + "\n" + match.group(3)
    
    new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    if new_content == content:
        print("âš ï¸ No status table found to update")
        return False
    
    # Add timestamp comment
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')
    new_content = new_content.replace(
        "*Build times measured on Apple Silicon M1",
        f"*Last updated: {timestamp} - Build times measured on GitHub Actions runner"
    )
    
    # Write updated README
    readme_path.write_text(new_content)
    print("âœ… README.md status table updated successfully")
    return True

if __name__ == "__main__":
    success = update_readme_status()
    exit(0 if success else 1)
EOF
          
          # Run the update script
          python3 update_readme_status.py
          UPDATE_SUCCESS=$?
          
          # Check if README was actually modified
          if git diff --quiet README.md; then
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ README.md was not modified"
          else
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "âœ… README.md has been updated"
            
            # Show the diff
            echo "=== README Changes ==="
            git diff README.md
          fi

  release:
    name: Create Release
    runs-on: ubuntu-latest
    needs: [detect-changes, validation, benchmark, update-readme]
    if: needs.update-readme.outputs.readme-changed == 'true' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download benchmark artifacts
        uses: actions/download-artifact@v3
        with:
          name: benchmark-reports-${{ github.sha }}

      - name: Determine version bump
        id: version
        run: |
          # Get current version from git tags
          CURRENT_VERSION=$(git tag --sort=-version:refname | grep -E '^v[0-9]+\.[0-9]+\.[0-9]+$' | head -1)
          
          if [[ -z "$CURRENT_VERSION" ]]; then
            CURRENT_VERSION="v0.0.0"
            echo "No previous version tags found, starting from $CURRENT_VERSION"
          fi
          
          echo "current_version=$CURRENT_VERSION" >> $GITHUB_OUTPUT
          
          # Determine version bump type
          VERSION_TYPE="${{ github.event.inputs.version_type }}"
          
          # Auto-determine version type if not manually specified
          if [[ -z "$VERSION_TYPE" ]]; then
            EXCELLENT_COUNT="${{ needs.validation.outputs.excellent-count }}"
            NEEDS_WORK_COUNT="${{ needs.validation.outputs.needs-work-count }}"
            
            if [[ $NEEDS_WORK_COUNT -eq 0 && $EXCELLENT_COUNT -gt 10 ]]; then
              VERSION_TYPE="minor"  # Significant improvement
            else
              VERSION_TYPE="patch"  # Regular update
            fi
          fi
          
          echo "version_type=$VERSION_TYPE" >> $GITHUB_OUTPUT
          
          # Calculate new version
          IFS='.' read -r -a version_parts <<< "${CURRENT_VERSION#v}"
          MAJOR=${version_parts[0]:-0}
          MINOR=${version_parts[1]:-0}
          PATCH=${version_parts[2]:-0}
          
          case $VERSION_TYPE in
            major)
              MAJOR=$((MAJOR + 1))
              MINOR=0
              PATCH=0
              ;;
            minor)
              MINOR=$((MINOR + 1))
              PATCH=0
              ;;
            patch)
              PATCH=$((PATCH + 1))
              ;;
          esac
          
          NEW_VERSION="v$MAJOR.$MINOR.$PATCH"
          echo "new_version=$NEW_VERSION" >> $GITHUB_OUTPUT
          
          echo "=== Version Information ==="
          echo "Current version: $CURRENT_VERSION"
          echo "Version bump type: $VERSION_TYPE"
          echo "New version: $NEW_VERSION"

      - name: Commit and push changes
        id: commit
        if: needs.update-readme.outputs.readme-changed == 'true'
        run: |
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Add benchmark reports
          git add benchmark_reports/ || true
          
          # Add updated README
          git add README.md
          
          # Create commit message
          TEST_MODE="${{ needs.detect-changes.outputs.test-mode }}"
          CHANGED_IMPLS="${{ needs.detect-changes.outputs.changed-implementations }}"
          
          if [[ "$TEST_MODE" == "selective" ]]; then
            COMMIT_MESSAGE="chore: update status for changed implementations ($CHANGED_IMPLS)

Selective benchmark results:
- Changed implementations: $CHANGED_IMPLS
- Overall status: ${{ needs.validation.outputs.excellent-count }} excellent, ${{ needs.validation.outputs.good-count }} good, ${{ needs.validation.outputs.needs-work-count }} needs work

Performance testing completed for modified implementations only."
          else
            COMMIT_MESSAGE="chore: update implementation status from benchmark suite

Benchmark results summary:
- Total implementations: ${{ needs.validation.outputs.total-count }}
- ðŸŸ¢ Excellent: ${{ needs.validation.outputs.excellent-count }}
- ðŸŸ¡ Good: ${{ needs.validation.outputs.good-count }}  
- ðŸ”´ Needs work: ${{ needs.validation.outputs.needs-work-count }}

Full performance testing completed with status updates."
          fi
          
          git commit -m "$COMMIT_MESSAGE"
          git push origin master
          
          echo "âœ… Changes committed and pushed"
          echo "commit_created=true" >> $GITHUB_OUTPUT

      - name: Create release tag
        id: tag
        run: |
          NEW_VERSION="${{ steps.version.outputs.new_version }}"
          
          # Create tag with release notes
          TAG_MESSAGE="Release $NEW_VERSION - Benchmark Update

## Implementation Status
- ðŸŸ¢ Excellent: ${{ needs.validation.outputs.excellent-count }} implementations
- ðŸŸ¡ Good: ${{ needs.validation.outputs.good-count }} implementations
- ðŸ”´ Needs work: ${{ needs.validation.outputs.needs-work-count }} implementations

## Benchmark Summary
- Total implementations: ${{ needs.validation.outputs.total-count }}
- Benchmark suite completed successfully
- README status table updated with latest performance data

Generated automatically by the benchmark suite workflow."

          git tag -a "$NEW_VERSION" -m "$TAG_MESSAGE"
          git push origin "$NEW_VERSION"
          
          echo "âœ… Release tag $NEW_VERSION created and pushed"
          echo "tag_created=true" >> $GITHUB_OUTPUT
          echo "tag_name=$NEW_VERSION" >> $GITHUB_OUTPUT

      - name: Create GitHub Release
        if: steps.tag.outputs.tag_created == 'true'
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ steps.tag.outputs.tag_name }}
          release_name: "Chess Engine Implementations ${{ steps.tag.outputs.tag_name }}"
          body: |
            # Chess Engine Implementation Benchmark Release ${{ steps.tag.outputs.tag_name }}
            
            This release contains updated performance benchmarks and status information for all chess engine implementations.
            
            ## ðŸ“Š Implementation Status Overview
            
            - ðŸŸ¢ **Excellent**: ${{ needs.validation.outputs.excellent-count }} implementations
            - ðŸŸ¡ **Good**: ${{ needs.validation.outputs.good-count }} implementations  
            - ðŸ”´ **Needs Work**: ${{ needs.validation.outputs.needs-work-count }} implementations
            
            **Total**: ${{ needs.validation.outputs.total-count }} implementations tested
            
            ## ðŸš€ What's Updated
            
            - âœ… Complete performance benchmark suite executed
            - âœ… Implementation structure verification completed
            - âœ… README status table updated with latest results
            - âœ… Docker build and test validation
            - âœ… Static analysis and build timing measurements
            
            ## ðŸ“‹ Benchmark Reports
            
            Detailed benchmark reports are available as workflow artifacts.
            
            ---
            
            *This release was automatically generated by the benchmark suite workflow.*
          draft: false
          prerelease: false

  summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [detect-changes, validation, benchmark, update-readme, release]
    if: always()
    
    steps:
      - name: Workflow summary
        run: |
          echo "# ðŸ Benchmark Suite Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          TEST_MODE="${{ needs.detect-changes.outputs.test-mode }}"
          HAS_CHANGES="${{ needs.detect-changes.outputs.has-changes }}"
          
          echo "## ðŸ” Workflow Mode" >> $GITHUB_STEP_SUMMARY
          if [[ "$HAS_CHANGES" == "false" ]]; then
            echo "- **Mode**: No changes detected" >> $GITHUB_STEP_SUMMARY
            echo "- **Action**: Workflow skipped" >> $GITHUB_STEP_SUMMARY
          elif [[ "$TEST_MODE" == "selective" ]]; then
            echo "- **Mode**: Selective benchmarking" >> $GITHUB_STEP_SUMMARY
            echo "- **Changed implementations**: ${{ needs.detect-changes.outputs.changed-implementations }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Mode**: Full benchmark suite" >> $GITHUB_STEP_SUMMARY
            echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "$HAS_CHANGES" == "true" ]]; then
            echo "## ðŸ“Š Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Total implementations**: ${{ needs.validation.outputs.total-count }}" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸŸ¢ **Excellent**: ${{ needs.validation.outputs.excellent-count }}" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸŸ¡ **Good**: ${{ needs.validation.outputs.good-count }}" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ”´ **Needs work**: ${{ needs.validation.outputs.needs-work-count }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "## ðŸ”„ Workflow Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Structure verification**: âœ… Completed" >> $GITHUB_STEP_SUMMARY
            echo "- **Performance benchmarks**: $( [[ '${{ needs.benchmark.outputs.benchmark-success }}' == 'true' ]] && echo 'âœ… Completed' || echo 'âŒ Failed' )" >> $GITHUB_STEP_SUMMARY
            echo "- **README updated**: $( [[ '${{ needs.update-readme.outputs.readme-changed }}' == 'true' ]] && echo 'âœ… Yes' || echo 'âš ï¸ No changes' )" >> $GITHUB_STEP_SUMMARY
            echo "- **Release created**: $( [[ '${{ needs.release.outputs.tag_created }}' == 'true' ]] && echo 'âœ… ${{ needs.release.outputs.tag_name }}' || echo 'âš ï¸ Not created' )" >> $GITHUB_STEP_SUMMARY
          fi